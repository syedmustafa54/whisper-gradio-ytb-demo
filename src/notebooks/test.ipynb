{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68970446-5fee-4ca7-83c8-4845f60bfb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "import time\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import gradio as gr\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import yt_dlp as youtube_dl\n",
    "from jax.experimental.compilation_cache import compilation_cache as cc\n",
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "from transformers.pipelines.audio_utils import ffmpeg_read\n",
    "\n",
    "from whisper_jax import FlaxWhisperPipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eda726c-5763-43b3-84de-a46e1b1fa5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized persistent compilation cache at ./jax_cache\n"
     ]
    }
   ],
   "source": [
    "cc.initialize_cache(\"./jax_cache\")\n",
    "checkpoint = \"openai/whisper-tiny\"\n",
    "\n",
    "DEBUG = True\n",
    "BATCH_SIZE = 32\n",
    "CHUNK_LENGTH_S = 30\n",
    "NUM_PROC = 32\n",
    "FILE_LIMIT_MB = 100000\n",
    "YT_LENGTH_LIMIT_S = 720000  # limit to 2 hour YouTube files\n",
    "\n",
    "title = description = article = \" Whisper JAX ⚡️ \"\n",
    "\n",
    "language_names = sorted(TO_LANGUAGE_CODE.keys())\n",
    "\n",
    "logger = logging.getLogger(\"whisper-jax-app\")\n",
    "logger.setLevel(logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "temp_path_zip_file = os.path.join(\"/home/ubuntu/whisper-gradio-ytb-demo/src\", 'temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca2fea1-63c5-4cd6-baa9-19733ccf0ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 13:10:14;INFO;compiling forward call...\n",
      "2023-05-05 13:10:17.476211: E external/xla/xla/stream_executor/cuda/cuda_blas.cc:190] failed to create cublas handle: cublas error\n",
      "2023-05-05 13:10:17.476255: E external/xla/xla/stream_executor/cuda/cuda_blas.cc:193] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2023-05-05 13:10:17.594513: W external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:850] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.\n",
      "2023-05-05 13:10:17.594546: W external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:853] Conv: (f32[32,384,3000]{2,1,0}, u8[0]{0}) custom-call(f32[32,80,3000]{2,1,0}, f32[384,80,3]{2,1,0}), window={size=3 pad=1_1}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv.2 = (f32[32,384,3000]{2,1,0}, u8[0]{0}) custom-call(f32[32,80,3000]{2,1,0} %Arg_167.168, f32[384,80,3]{2,1,0} %transpose.405), window={size=3 pad=1_1}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"pmap(generate)/jit(main)/encoder/conv1/conv_general_dilated[window_strides=(1,) padding=((1, 1),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_44303/3957122359.py\" source_line=12}, backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\n\nOriginal error: INTERNAL: All algorithms tried for %cudnn-conv.2 = (f32[32,384,3000]{2,1,0}, u8[0]{0}) custom-call(f32[32,80,3000]{2,1,0} %Arg_167.168, f32[384,80,3]{2,1,0} %transpose.405), window={size=3 pad=1_1}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"pmap(generate)/jit(main)/encoder/conv1/conv_general_dilated[window_strides=(1,) padding=((1, 1),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_44303/3957122359.py\" source_line=12}, backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n  Profiling failure on cuDNN engine eng34{k2=0,k4=2,k5=1,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng34{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng4{}: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng34{k2=2,k4=1,k5=0,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng42{k2=1,k4=1,k5=1,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng31{k2=2,k4=2,k5=3,k6=2,k7=1}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng34{k2=2,k4=2,k5=0,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng30{k2=2,k4=2,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng42{k2=2,k4=1,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng1{k2=2,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{k2=1,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng2{k2=1,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng1{k2=4,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{k2=0,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng2{k2=3,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{k2=3,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng1{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m random_inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mones((BATCH_SIZE, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m3000\u001b[39m))}\n\u001b[0;32m---> 12\u001b[0m random_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompile_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/whipser-gradio/lib/python3.9/site-packages/whisper_jax/pipeline.py:410\u001b[0m, in \u001b[0;36mFlaxWhisperPipline.forward\u001b[0;34m(self, model_inputs, batch_size, language, task, return_timestamps)\u001b[0m\n\u001b[1;32m    407\u001b[0m     padding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([batch_size \u001b[38;5;241m-\u001b[39m input_batch_size, \u001b[38;5;241m*\u001b[39minput_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]], input_features\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    408\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([input_features, padding])\n\u001b[0;32m--> 410\u001b[0m pred_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_timestamps\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    411\u001b[0m     :input_batch_size\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# tokenizer's decode method expects an extra dim - we insert it here for convenience\u001b[39;00m\n\u001b[1;32m    415\u001b[0m out \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: pred_ids[:, \u001b[38;5;28;01mNone\u001b[39;00m, :]}\n",
      "File \u001b[0;32m/opt/conda/envs/whipser-gradio/lib/python3.9/site-packages/whisper_jax/pipeline.py:190\u001b[0m, in \u001b[0;36mFlaxWhisperPipline.generate\u001b[0;34m(self, input_features, language, task, return_timestamps)\u001b[0m\n\u001b[1;32m    185\u001b[0m forced_decoder_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_forced_decoder_ids(\n\u001b[1;32m    186\u001b[0m     language\u001b[38;5;241m=\u001b[39mlanguage, task\u001b[38;5;241m=\u001b[39mtask, return_timestamps\u001b[38;5;241m=\u001b[39mreturn_timestamps\n\u001b[1;32m    187\u001b[0m )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sharded:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# if we're using pmap we need to manually replicate the input data across devices and gather the output tokens\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_decoder_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msequences\n\u001b[1;32m    193\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_get(output_ids\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length))\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# pjit handles replication / gathering for us auto-magically\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/whipser-gradio/lib/python3.9/site-packages/jax/_src/dispatch.py:471\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, built_c, options, host_callbacks)\u001b[0m\n\u001b[1;32m    466\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    467\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n%cudnn-conv.2 = (f32[32,384,3000]{2,1,0}, u8[0]{0}) custom-call(f32[32,80,3000]{2,1,0} %Arg_167.168, f32[384,80,3]{2,1,0} %transpose.405), window={size=3 pad=1_1}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"pmap(generate)/jit(main)/encoder/conv1/conv_general_dilated[window_strides=(1,) padding=((1, 1),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_44303/3957122359.py\" source_line=12}, backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\n\nOriginal error: INTERNAL: All algorithms tried for %cudnn-conv.2 = (f32[32,384,3000]{2,1,0}, u8[0]{0}) custom-call(f32[32,80,3000]{2,1,0} %Arg_167.168, f32[384,80,3]{2,1,0} %transpose.405), window={size=3 pad=1_1}, dim_labels=bf0_oi0->bf0, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"pmap(generate)/jit(main)/encoder/conv1/conv_general_dilated[window_strides=(1,) padding=((1, 1),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_44303/3957122359.py\" source_line=12}, backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\" failed. Falling back to default algorithm.  Per-algorithm errors:\n  Profiling failure on cuDNN engine eng34{k2=0,k4=2,k5=1,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng34{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng4{}: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng34{k2=2,k4=1,k5=0,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng42{k2=1,k4=1,k5=1,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng31{k2=2,k4=2,k5=3,k6=2,k7=1}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng34{k2=2,k4=2,k5=0,k6=0,k7=0,k19=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng30{k2=2,k4=2,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng42{k2=2,k4=1,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng1{k2=2,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{k2=1,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng2{k2=1,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng1{k2=4,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{k2=0,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng2{k2=3,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{k2=3,k3=0}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng1{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n  Profiling failure on cuDNN engine eng28{}: UNKNOWN: CUDNN_STATUS_ALLOC_FAILED\nin external/xla/xla/stream_executor/cuda/cuda_dnn.cc(4686): 'status'\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning."
     ]
    }
   ],
   "source": [
    "pipeline = FlaxWhisperPipline(checkpoint, dtype=jnp.bfloat16, batch_size=BATCH_SIZE)\n",
    "stride_length_s = CHUNK_LENGTH_S / 6\n",
    "chunk_len = round(CHUNK_LENGTH_S * pipeline.feature_extractor.sampling_rate)\n",
    "stride_left = stride_right = round(stride_length_s * pipeline.feature_extractor.sampling_rate)\n",
    "step = chunk_len - stride_left - stride_right\n",
    "pool = Pool(NUM_PROC)\n",
    "\n",
    "#do a pre-compile step so that the first user to use the demo isn't hit with a long transcription time\n",
    "logger.info(\"compiling forward call...\")\n",
    "start = time.time()\n",
    "random_inputs = {\"input_features\": np.ones((BATCH_SIZE, 80, 3000))}\n",
    "random_timestamps = pipeline.forward(random_inputs, batch_size=BATCH_SIZE, return_timestamps=True)\n",
    "compile_time = time.time() - start\n",
    "logger.info(f\"compiled in {compile_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f8010-29e0-4d3f-a124-ac3efd2ae2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identity(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b24679-51f6-4ad3-ab4f-e793f825fb7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3628bf09-34d4-4e64-abd6-3dc10bb6891c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"):\n",
    "    if seconds is None:\n",
    "        # we have a malformed timestamp so just return it as is\n",
    "        return seconds\n",
    "    milliseconds = round(seconds * 1000.0)\n",
    "\n",
    "    hours = milliseconds // 3_600_000\n",
    "    milliseconds -= hours * 3_600_000\n",
    "\n",
    "    minutes = milliseconds // 60_000\n",
    "    milliseconds -= minutes * 60_000\n",
    "\n",
    "    seconds = milliseconds // 1_000\n",
    "    milliseconds -= seconds * 1_000\n",
    "\n",
    "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
    "    return f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
    "\n",
    "def create_transcript_zip(videos,tmpdir):\n",
    "        \"\"\"\n",
    "      Clear the temporary directory contents\n",
    "      \n",
    "      Create a zip file for each video transcript and return the path to the zip of all transcripts.\n",
    "\n",
    "      Args:\n",
    "      videos (list of dict): Each dictionary must have \"title\" and \"transcript\" keys, containing the video title\n",
    "      and its transcript respectively.\n",
    "\n",
    "      Returns:\n",
    "      str: Path to the zip file containing all transcript zip files.\n",
    "      \"\"\"\n",
    "        for filename in os.listdir(tmpdir):\n",
    "            file_path = os.path.join(tmpdir, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "                print(f'Deleted {file_path}')\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "        # Create a temporary directory to store all transcript zip files\n",
    "        zip_paths = []\n",
    "        # Loop through all videos and create a transcript zip file for each\n",
    "        for video in videos:\n",
    "            # Create a zip file with the video title as the filename\n",
    "            zip_path = os.path.join(tmpdir, f\"{video['title']}.zip\")\n",
    "            if not os.path.exists(temp_path_zip_file):\n",
    "              os.makedirs(temp_path_zip_file)\n",
    "            with zipfile.ZipFile(zip_path, \"w\") as zip_file:\n",
    "                # Write the transcript to an SRT file with the same name as the video\n",
    "                srt_path = os.path.join(tmpdir, f\"{video['title']}.srt\")\n",
    "                with open(srt_path, \"w\") as srt_file:\n",
    "                    srt_file.write(video[\"transcript\"])\n",
    "                # Add the SRT file to the zip\n",
    "                zip_file.write(srt_path, f\"{video['title']}.srt\")\n",
    "            zip_paths.append(zip_path)\n",
    "        # Create a zip file containing all transcript zip files\n",
    "        all_zip_path = os.path.join(tmpdir, \"all_transcripts.zip\")\n",
    "        with zipfile.ZipFile(all_zip_path, \"w\") as all_zip_file:\n",
    "            for zip_path in zip_paths:\n",
    "                all_zip_file.write(zip_path, os.path.basename(zip_path))\n",
    "        return all_zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36adcd08-d7ad-420f-be11-5507649c1feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _return_yt_html_embed(yt_url):\n",
    "    video_id = yt_url[-1].split(\"?v=\")[-1]\n",
    "    return f'<center> <iframe width=\"500\" height=\"320\" src=\"https://www.youtube.com/embed/{video_id}\"> </iframe> </center>'\n",
    "\n",
    "def download_yt_audio(yt_url, filename):\n",
    "    title_ytb = youtube_dl.YoutubeDL().extract_info(yt_url, download=False).get(\"title\", None)\n",
    "    info_loader = youtube_dl.YoutubeDL()\n",
    "    try:\n",
    "        info = info_loader.extract_info(yt_url, download=False)\n",
    "    except youtube_dl.utils.DownloadError as err:\n",
    "        raise gr.Error(str(err)) from err\n",
    "\n",
    "    file_length = info[\"duration_string\"]\n",
    "    file_h_m_s = file_length.split(\":\")\n",
    "    file_h_m_s = [int(sub_length) for sub_length in file_h_m_s]\n",
    "    if len(file_h_m_s) == 1:\n",
    "        file_h_m_s.insert(0, 0)\n",
    "    if len(file_h_m_s) == 2:\n",
    "        file_h_m_s.insert(0, 0)\n",
    "\n",
    "    file_length_s = file_h_m_s[0] * 3600 + file_h_m_s[1] * 60 + file_h_m_s[2]\n",
    "    if file_length_s > YT_LENGTH_LIMIT_S:\n",
    "        yt_length_limit_hms = time.strftime(\"%HH:%MM:%SS\", time.gmtime(YT_LENGTH_LIMIT_S))\n",
    "        file_length_hms = time.strftime(\"%HH:%MM:%SS\", time.gmtime(file_length_s))\n",
    "        raise gr.Error(f\"Maximum YouTube length is {yt_length_limit_hms}, got {file_length_hms} YouTube video.\")\n",
    "\n",
    "    ydl_opts = {\"outtmpl\": filename, \"format\": \"worstvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best\"}\n",
    "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            ydl.download([yt_url])\n",
    "            return title_ytb\n",
    "        except youtube_dl.utils.ExtractorError as err:\n",
    "            raise gr.Error(str(err)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562b444-1ed1-4f66-8049-f1f290fdb72f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eac70db-4100-4807-955a-c43bfa2813e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tqdm_generate(inputs: dict, task: str, return_timestamps: bool):\n",
    "    inputs_len = inputs[\"array\"].shape[0]\n",
    "    all_chunk_start_idx = np.arange(0, inputs_len, step)\n",
    "    num_samples = len(all_chunk_start_idx)\n",
    "    num_batches = math.ceil(num_samples / BATCH_SIZE)\n",
    "    dummy_batches = list(\n",
    "        range(num_batches)\n",
    "    )  # Gradio progress bar not compatible with generator, see https://github.com/gradio-app/gradio/issues/3841\n",
    "\n",
    "    dataloader = pipeline.preprocess_batch(inputs, chunk_length_s=CHUNK_LENGTH_S, batch_size=BATCH_SIZE)\n",
    "    #progress(0, desc=\"Pre-processing audio file...\")\n",
    "    logger.info(\"pre-processing audio file...\")\n",
    "    dataloader = pool.map(identity, dataloader)\n",
    "    logger.info(\"done post-processing\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.info(\"transcribing...\")\n",
    "    model_outputs = [\n",
    "        pipeline.forward(\n",
    "            batch, batch_size=BATCH_SIZE, task=task, return_timestamps=True\n",
    "        )\n",
    "        for batch, _ in zip(\n",
    "            dataloader\n",
    "        )\n",
    "    ]\n",
    "    runtime = time.time() - start_time\n",
    "    logger.info(\"done transcription\")\n",
    "\n",
    "    logger.info(\"post-processing...\")\n",
    "    post_processed = pipeline.postprocess(model_outputs, return_timestamps=True)\n",
    "    text = post_processed[\"text\"]\n",
    "    if return_timestamps:\n",
    "        timestamps = post_processed.get(\"chunks\")\n",
    "        timestamps = [\n",
    "            f\"[{format_timestamp(chunk['timestamp'][0])} -> {format_timestamp(chunk['timestamp'][1])}] {chunk['text']}\"\n",
    "            for chunk in timestamps\n",
    "        ]\n",
    "        text = \"\\n\".join(str(feature) for feature in timestamps)\n",
    "    logger.info(\"done post-processing\")\n",
    "    return text, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6baa9e7-8683-486d-bba7-4dafa70290de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe_chunked_audio(inputs, task, return_timestamps):\n",
    "    #progress(0, desc=\"Loading audio file...\")\n",
    "    logger.info(\"loading audio file...\")\n",
    "    if inputs is None:\n",
    "        logger.warning(\"No audio file\")\n",
    "        raise gr.Error(\"No audio file submitted! Please upload an audio file before submitting your request.\")\n",
    "    file_size_mb = os.stat(inputs).st_size / (1024 * 1024)\n",
    "    if file_size_mb > FILE_LIMIT_MB:\n",
    "        logger.warning(\"Max file size exceeded\")\n",
    "        raise gr.Error(\n",
    "            f\"File size exceeds file size limit. Got file of size {file_size_mb:.2f}MB for a limit of {FILE_LIMIT_MB}MB.\"\n",
    "        )\n",
    "\n",
    "    with open(inputs, \"rb\") as f:\n",
    "        inputs = f.read()\n",
    "\n",
    "    inputs = ffmpeg_read(inputs, pipeline.feature_extractor.sampling_rate)\n",
    "    sampling_rate = float(pipeline.feature_extractor.sampling_rate)\n",
    "    inputs = {\"array\": inputs, \"sampling_rate\": sampling_rate}\n",
    "    logger.info(\"done loading\")\n",
    "    text, runtime = tqdm_generate(inputs, task=task, return_timestamps=return_timestamps)\n",
    "    return text, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad9fbfb-6e56-4d75-8cff-59eb8496716c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def transcribe_youtube(yt_urls, task, return_timestamps):\n",
    "        final_files_data = []\n",
    "        yt_urls = yt_urls.split()\n",
    "        html_embed_str = _return_yt_html_embed(yt_urls)\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            print(yt_urls)\n",
    "            for yt_url in yt_urls:\n",
    "                ran_id = str(uuid.uuid4())\n",
    "                filepath = os.path.join(tmpdirname, f\"{ran_id}_video.mp4\")\n",
    "                print(f\"///////////----{filepath}\")\n",
    "                print(f\"\\n--Doing for {yt_urls.index(yt_url)}--{filepath}----\\n\")\n",
    "                title_ytb = download_yt_audio(yt_url, filepath)\n",
    "\n",
    "                with open(filepath, \"rb\") as f:\n",
    "                    inputs = f.read()\n",
    "\n",
    "        #         inputs = ffmpeg_read(inputs, pipeline.feature_extractor.sampling_rate)\n",
    "        #         inputs = {\"array\": inputs, \"sampling_rate\": pipeline.feature_extractor.sampling_rate}\n",
    "        #         logger.info(\"done loading...\")\n",
    "        #         text, runtime = tqdm_generate(inputs, task=task, return_timestamps=return_timestamps)\n",
    "        #         final_files_data.append({\"title\": title_ytb, \"transcript\": text})\n",
    "        # path_of_zip_file = create_transcript_zip(final_files_data, temp_path_zip_file)\n",
    "        # return html_embed_str, path_of_zip_file, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e2e899e-7c72-4bc9-83be-9a73399f8188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yt_urls = \"https://www.youtube.com/watch?v=4AHz39IIkmc https://www.youtube.com/watch?v=vhr-i1WtfXY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747639d8-4d11-42b3-a625-e0668fbb45d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=4AHz39IIkmc', 'https://www.youtube.com/watch?v=vhr-i1WtfXY']\n",
      "///////////----/tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.mp4\n",
      "\n",
      "--Doing for 0--/tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.mp4----\n",
      "\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=4AHz39IIkmc\n",
      "[youtube] 4AHz39IIkmc: Downloading webpage\n",
      "[youtube] 4AHz39IIkmc: Downloading android player API JSON\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=4AHz39IIkmc\n",
      "[youtube] 4AHz39IIkmc: Downloading webpage\n",
      "[youtube] 4AHz39IIkmc: Downloading android player API JSON\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=4AHz39IIkmc\n",
      "[youtube] 4AHz39IIkmc: Downloading webpage\n",
      "[youtube] 4AHz39IIkmc: Downloading android player API JSON\n",
      "[info] 4AHz39IIkmc: Downloading 1 format(s): 597+140\n",
      "[dashsegments] Total fragments: 1\n",
      "[download] Destination: /tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.f597.mp4\n",
      "[download] 100% of    2.08MiB in 00:00:00 at 11.25MiB/s              \n",
      "[dashsegments] Total fragments: 1\n",
      "[download] Destination: /tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.f140.m4a\n",
      "[download] 100% of    8.59MiB in 00:00:00 at 35.36MiB/s              \n",
      "[Merger] Merging formats into \"/tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.mp4\"\n",
      "Deleting original file /tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.f140.m4a (pass -k to keep)\n",
      "Deleting original file /tmp/tmp5zqrooq3/ed7423e2-37c9-4039-9b2d-24c5aaa430cd_video.f597.mp4 (pass -k to keep)\n",
      "///////////----/tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.mp4\n",
      "\n",
      "--Doing for 1--/tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.mp4----\n",
      "\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vhr-i1WtfXY\n",
      "[youtube] vhr-i1WtfXY: Downloading webpage\n",
      "[youtube] vhr-i1WtfXY: Downloading android player API JSON\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vhr-i1WtfXY\n",
      "[youtube] vhr-i1WtfXY: Downloading webpage\n",
      "[youtube] vhr-i1WtfXY: Downloading android player API JSON\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vhr-i1WtfXY\n",
      "[youtube] vhr-i1WtfXY: Downloading webpage\n",
      "[youtube] vhr-i1WtfXY: Downloading android player API JSON\n",
      "[info] vhr-i1WtfXY: Downloading 1 format(s): 597+140\n",
      "[dashsegments] Total fragments: 1\n",
      "[download] Destination: /tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.f597.mp4\n",
      "[download] 100% of    2.67MiB in 00:00:00 at 15.06MiB/s              \n",
      "[dashsegments] Total fragments: 2\n",
      "[download] Destination: /tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.f140.m4a\n",
      "[download] 100% of   10.43MiB in 00:00:00 at 26.47MiB/s              \n",
      "[Merger] Merging formats into \"/tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.mp4\"\n",
      "Deleting original file /tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.f140.m4a (pass -k to keep)\n",
      "Deleting original file /tmp/tmp5zqrooq3/8b953e52-2858-4698-b170-32236f57209a_video.f597.mp4 (pass -k to keep)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m html_embed_str, path_of_zip_file, runtime \u001b[38;5;241m=\u001b[39m transcribe_youtube(yt_urls,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "html_embed_str, path_of_zip_file, runtime = transcribe_youtube(yt_urls,\"transcribe\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6cc1a-8aca-4213-aa82-2371f94fbf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eab95d-0f99-4918-a793-8f5c23b405d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220f81d-3ab3-48b3-a9ae-2f7464ca6121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
